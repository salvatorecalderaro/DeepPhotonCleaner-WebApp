from torch import nn

class MultichannelAutoencoder(nn.Module):
    def __init__(self):
        """
        Initializes a MultichannelAutoencoder.

        The autoencoder uses a convolutional encoder and decoder, with a
        bottleneck in the middle. The output of the autoencoder is passed
        through a Softplus activation function to ensure it is positive.
        """
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv1d(2, 32, kernel_size=5, padding=2),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(2),  # Down to 1/2
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),  # Down to 1/4
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),  # Down to 1/8
        )

        # Bottleneck (latent representation)
        self.bottleneck = nn.Sequential(
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Conv1d(256, 128, kernel_size=3, padding=1),
            nn.ReLU(),
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.ConvTranspose1d(32, 2, kernel_size=4, stride=2, padding=1),
            nn.Softplus(),
        )

    def forward(self, x):
        """
        Defines the computation performed at every call.

        Should be overridden by all subclasses.
        """
        encoded = self.encoder(x)
        latent = self.bottleneck(encoded)
        decoded = self.decoder(latent)
        return decoded,latent